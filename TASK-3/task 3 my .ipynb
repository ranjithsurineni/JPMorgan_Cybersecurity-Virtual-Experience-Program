{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1df29eda",
   "metadata": {
    "id": "1df29eda"
   },
   "source": [
    "Step 0. Unzip enron1.zip into the current directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf32cfce",
   "metadata": {
    "id": "bf32cfce"
   },
   "source": [
    "Step 1. Traverse the dataset and create a Pandas dataframe. This is already done for you and should run without any errors. You should recognize Pandas from task 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20c5d195",
   "metadata": {
    "id": "20c5d195"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped 0104.2003-12-29.GP.spam.txt\n",
      "skipped 0135.2004-01-04.GP.spam.txt\n",
      "skipped 0176.2004-01-11.GP.spam.txt\n",
      "skipped 0202.2004-01-13.GP.spam.txt\n",
      "skipped 0256.2004-01-20.GP.spam.txt\n",
      "skipped 0307.2004-01-24.GP.spam.txt\n",
      "skipped 0328.2004-01-29.GP.spam.txt\n",
      "skipped 0391.2004-02-08.GP.spam.txt\n",
      "skipped 0407.2004-02-11.GP.spam.txt\n",
      "skipped 0555.2004-02-22.GP.spam.txt\n",
      "skipped 0578.2004-02-29.GP.spam.txt\n",
      "skipped 0588.2004-03-02.GP.spam.txt\n",
      "skipped 0647.2004-03-13.GP.spam.txt\n",
      "skipped 0726.2004-03-26.GP.spam.txt\n",
      "skipped 0758.2004-04-02.GP.spam.txt\n",
      "skipped 0849.2004-04-14.GP.spam.txt\n",
      "skipped 0862.2004-04-15.GP.spam.txt\n",
      "skipped 0871.2004-04-18.GP.spam.txt\n",
      "skipped 1023.2004-05-04.GP.spam.txt\n",
      "skipped 1040.2004-05-06.GP.spam.txt\n",
      "skipped 1107.2004-05-15.GP.spam.txt\n",
      "skipped 1180.2004-05-21.GP.spam.txt\n",
      "skipped 1218.2004-05-28.GP.spam.txt\n",
      "skipped 1295.2004-06-06.GP.spam.txt\n",
      "skipped 1301.2004-06-07.GP.spam.txt\n",
      "skipped 1309.2004-06-08.GP.spam.txt\n",
      "skipped 1313.2004-06-09.GP.spam.txt\n",
      "skipped 1340.2004-06-14.GP.spam.txt\n",
      "skipped 1378.2004-06-18.GP.spam.txt\n",
      "skipped 1437.2004-06-28.GP.spam.txt\n",
      "skipped 1452.2004-06-29.GP.spam.txt\n",
      "skipped 1469.2004-07-02.GP.spam.txt\n",
      "skipped 1473.2004-07-03.GP.spam.txt\n",
      "skipped 1489.2004-07-03.GP.spam.txt\n",
      "skipped 1581.2004-07-13.GP.spam.txt\n",
      "skipped 1656.2004-07-19.GP.spam.txt\n",
      "skipped 1717.2004-07-27.GP.spam.txt\n",
      "skipped 1844.2004-08-15.GP.spam.txt\n",
      "skipped 1863.2004-08-17.GP.spam.txt\n",
      "skipped 1960.2004-08-23.GP.spam.txt\n",
      "skipped 1993.2004-08-26.GP.spam.txt\n",
      "skipped 2030.2004-08-30.GP.spam.txt\n",
      "skipped 2055.2004-08-31.GP.spam.txt\n",
      "skipped 2075.2004-09-04.GP.spam.txt\n",
      "skipped 2111.2004-09-10.GP.spam.txt\n",
      "skipped 2167.2004-09-15.GP.spam.txt\n",
      "skipped 2242.2004-09-22.GP.spam.txt\n",
      "skipped 2290.2004-09-26.GP.spam.txt\n",
      "skipped 2353.2004-10-02.GP.spam.txt\n",
      "skipped 2377.2004-10-05.GP.spam.txt\n",
      "skipped 2430.2004-10-08.GP.spam.txt\n",
      "skipped 2455.2004-10-09.GP.spam.txt\n",
      "skipped 2581.2004-10-23.GP.spam.txt\n",
      "skipped 2649.2004-10-27.GP.spam.txt\n",
      "skipped 2713.2004-11-02.GP.spam.txt\n",
      "skipped 2756.2004-11-08.GP.spam.txt\n",
      "skipped 2794.2004-11-11.GP.spam.txt\n",
      "skipped 2807.2004-11-12.GP.spam.txt\n",
      "skipped 2888.2004-11-20.GP.spam.txt\n",
      "skipped 2957.2004-11-27.GP.spam.txt\n",
      "skipped 2987.2004-11-30.GP.spam.txt\n",
      "skipped 3035.2004-12-03.GP.spam.txt\n",
      "skipped 3042.2004-12-05.GP.spam.txt\n",
      "skipped 3072.2004-12-06.GP.spam.txt\n",
      "skipped 3165.2004-12-11.GP.spam.txt\n",
      "skipped 3173.2004-12-13.GP.spam.txt\n",
      "skipped 3302.2004-12-26.GP.spam.txt\n",
      "skipped 3312.2004-12-27.GP.spam.txt\n",
      "skipped 3315.2004-12-27.GP.spam.txt\n",
      "skipped 3343.2004-12-29.GP.spam.txt\n",
      "skipped 3383.2005-01-04.GP.spam.txt\n",
      "skipped 3384.2005-01-04.GP.spam.txt\n",
      "skipped 3455.2005-01-11.GP.spam.txt\n",
      "skipped 3530.2005-01-21.GP.spam.txt\n",
      "skipped 3562.2005-01-23.GP.spam.txt\n",
      "skipped 3591.2005-01-26.GP.spam.txt\n",
      "skipped 3651.2005-01-31.GP.spam.txt\n",
      "skipped 3668.2005-02-03.GP.spam.txt\n",
      "skipped 3687.2005-02-04.GP.spam.txt\n",
      "skipped 3709.2005-02-04.GP.spam.txt\n",
      "skipped 3714.2005-02-05.GP.spam.txt\n",
      "skipped 3821.2005-02-14.GP.spam.txt\n",
      "skipped 3844.2005-02-15.GP.spam.txt\n",
      "skipped 3858.2005-02-17.GP.spam.txt\n",
      "skipped 3921.2005-02-25.GP.spam.txt\n",
      "skipped 3973.2005-03-05.GP.spam.txt\n",
      "skipped 3979.2005-03-05.GP.spam.txt\n",
      "skipped 3988.2005-03-06.GP.spam.txt\n",
      "skipped 4102.2005-03-19.GP.spam.txt\n",
      "skipped 4113.2005-03-20.GP.spam.txt\n",
      "skipped 4132.2005-03-28.GP.spam.txt\n",
      "skipped 4152.2005-04-01.GP.spam.txt\n",
      "skipped 4314.2005-04-19.GP.spam.txt\n",
      "skipped 4363.2005-04-24.GP.spam.txt\n",
      "skipped 4373.2005-04-25.GP.spam.txt\n",
      "skipped 4382.2005-04-26.GP.spam.txt\n",
      "skipped 4412.2005-04-29.GP.spam.txt\n",
      "skipped 4420.2005-04-30.GP.spam.txt\n",
      "skipped 4428.2005-05-01.GP.spam.txt\n",
      "skipped 4455.2005-05-07.GP.spam.txt\n",
      "skipped 4472.2005-05-10.GP.spam.txt\n",
      "skipped 4486.2005-05-15.GP.spam.txt\n",
      "skipped 4497.2005-05-17.GP.spam.txt\n",
      "skipped 4577.2005-05-27.GP.spam.txt\n",
      "skipped 4665.2005-06-09.GP.spam.txt\n",
      "skipped 4687.2005-06-12.GP.spam.txt\n",
      "skipped 4735.2005-06-20.GP.spam.txt\n",
      "skipped 4743.2005-06-25.GP.spam.txt\n",
      "skipped 4755.2005-06-27.GP.spam.txt\n",
      "skipped 4766.2005-06-28.GP.spam.txt\n",
      "skipped 4796.2005-07-03.GP.spam.txt\n",
      "skipped 4935.2005-07-27.GP.spam.txt\n",
      "skipped 4979.2005-08-06.GP.spam.txt\n",
      "skipped 5020.2005-08-16.GP.spam.txt\n",
      "skipped 5032.2005-08-18.GP.spam.txt\n",
      "skipped 5057.2005-08-22.GP.spam.txt\n",
      "skipped 5079.2005-08-25.GP.spam.txt\n",
      "skipped 5110.2005-08-31.GP.spam.txt\n",
      "skipped 5132.2005-09-02.GP.spam.txt\n",
      "skipped 5145.2005-09-04.GP.spam.txt\n",
      "skipped 5155.2005-09-05.GP.spam.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def read_spam():\n",
    "    category = 'spam'\n",
    "    directory = 'C:/Users/ADMIN/Desktop/J.P.Morgan Cyber security/TASK-3/enron1/spam'\n",
    "    return read_category(category, directory)\n",
    "\n",
    "def read_ham():\n",
    "    category = 'ham'\n",
    "    directory = './enron1/ham'\n",
    "    return read_category(category, directory)\n",
    "\n",
    "def read_category(category, directory):\n",
    "    emails = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        with open(os.path.join(directory, filename), 'r') as fp:\n",
    "            try:\n",
    "                content = fp.read()\n",
    "                emails.concat({'name': filename, 'content': content, 'category': category})\n",
    "            except:\n",
    "                print(f'skipped {filename}')\n",
    "    return emails\n",
    "\n",
    "ham = read_ham()\n",
    "spam = read_spam()\n",
    "\n",
    "df = pd.DataFrame.from_records(ham)\n",
    "df1 = pd.DataFrame.from_records(spam)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1c23fd",
   "metadata": {
    "id": "1a1c23fd"
   },
   "source": [
    "Step 2. Data cleaning is a critical part of machine learning. You and I can recognize that 'Hello' and 'hello' are the same word but a machine does not know this a priori. Therefore, we can 'help' the machine by conducting such normalization steps for it. Write a function `preprocessor` that takes in a string and replaces all non alphabet characters with a space and then lowercases the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c447c901",
   "metadata": {
    "id": "c447c901"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocessor(e):\n",
    "    return re.sub('[^A-Za-z]',' ',e).lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba32521d",
   "metadata": {
    "id": "ba32521d"
   },
   "source": [
    "Step 3. We will now train the machine learning model. All the functions that you will need are imported for you. The instructions explain how the work and hint at which functions to use. You will likely need to refer to the scikit learn documentation to see how exactly to invoke the functions. It will be handy to keep that tab open."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1442d377",
   "metadata": {
    "id": "1442d377"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\"\\n# We now want to see how we have done. We will be using three functions.\\n# `accuracy_score` tells us how well we have done. \\n# 90% means that every 9 of 10 entries from the test dataset were predicted accurately.\\n# The `confusion_matrix` is a 2x2 matrix that gives us more insight.\\n# The top left shows us how many ham emails were predicted to be ham (that\\'s good!).\\n# The bottom right shows us how many spam emails were predicted to be spam (that\\'s good!).\\n# The other two quadrants tell us the misclassifications.\\n# Finally, the `classification_report` gives us detailed statistics which you may have seen in a statistics class.\\n# TODO\\nprint(f\\'Accuracy:\\n{accuracy_score(y_test,y_pred)}\\n\\')\\nprint(f\\'Confusion Matrix:\\n{confusion_matrix(y_test,y_pred)}\\n\\')\\nprint(f\\'Detailed Statistics:\\n{classification_report(y_test,y_pred)}\\n\\')\\n\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# The CountVectorizer converts a text sample into a vector (think of it as an array of floats).\n",
    "# Each entry in the vector corresponds to a single word and the value is the number of times the word appeared.\n",
    "# Instantiate a CountVectorizer. Make sure to include the preprocessor you previously wrote in the constructor.\n",
    "# TODO\n",
    "vectorizer = CountVectorizer(preprocessor=preprocessor)\n",
    "\n",
    "\n",
    "# Use train_test_split to split the dataset into a train dataset and a test dataset.\n",
    "# The machine learning model learns from the train dataset.\n",
    "# Then the trained model is tested on the test dataset to see if it actually learned anything.\n",
    "# If it just memorized for example, then it would have a low accuracy on the test dataset and a high accuracy on the train dataset.\n",
    "# TODO\n",
    "#X_train,X_test,y_train,y_test = train_test_split(df[\"content\"],df[\"category\"],test_size=0.2,random_state=1)\n",
    "\n",
    "\n",
    "\n",
    "# Use the vectorizer to transform the existing dataset into a form in which the model can learn from.\n",
    "# Remember that simple machine learning models operate on numbers, which the CountVectorizer conveniently helped us do.\n",
    "# TODO\n",
    "#X_train_df = vectorizer.fit_transform(X_train)\n",
    "\n",
    "\n",
    "# Use the LogisticRegression model to fit to the train dataset.\n",
    "# You may remember y = mx + b and Linear Regression from high school. Here, we fitted a scatter plot to a line.\n",
    "# Logistic Regression is another form of regression. \n",
    "# However, Logistic Regression helps us determine if a point should be in category A or B, which is a perfect fit.\n",
    "# TODO\n",
    "#model = LogisticRegression()\n",
    "#model.fit(X_train_df,y_train)\n",
    "\n",
    "\n",
    "\n",
    "# Validate that the model has learned something.\n",
    "# Recall the model operates on vectors. First transform the test set using the vectorizer. \n",
    "# Then generate the predictions.\n",
    "# TODO\n",
    "#X_test_df = vectorizer.transform(X_test)\n",
    "#y_pred = model.predict(X_test_df)\n",
    "\n",
    "\n",
    "\"\"\"\"\"\n",
    "# We now want to see how we have done. We will be using three functions.\n",
    "# `accuracy_score` tells us how well we have done. \n",
    "# 90% means that every 9 of 10 entries from the test dataset were predicted accurately.\n",
    "# The `confusion_matrix` is a 2x2 matrix that gives us more insight.\n",
    "# The top left shows us how many ham emails were predicted to be ham (that's good!).\n",
    "# The bottom right shows us how many spam emails were predicted to be spam (that's good!).\n",
    "# The other two quadrants tell us the misclassifications.\n",
    "# Finally, the `classification_report` gives us detailed statistics which you may have seen in a statistics class.\n",
    "# TODO\n",
    "print(f'Accuracy:\\n{accuracy_score(y_test,y_pred)}\\n')\n",
    "print(f'Confusion Matrix:\\n{confusion_matrix(y_test,y_pred)}\\n')\n",
    "print(f'Detailed Statistics:\\n{classification_report(y_test,y_pred)}\\n')\n",
    "\n",
    "\"\"\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9674d032",
   "metadata": {
    "id": "9674d032"
   },
   "source": [
    "Step 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7d78c9",
   "metadata": {
    "id": "6b7d78c9"
   },
   "outputs": [],
   "source": [
    "# Let's see which features (aka columns) the vectorizer created. \n",
    "# They should be all the words that were contained in the training dataset.\n",
    "# TODO\n",
    "\n",
    "\n",
    "# You may be wondering what a machine learning model is tangibly. It is just a collection of numbers. \n",
    "# You can access these numbers known as \"coefficients\" from the coef_ property of the model\n",
    "# We will be looking at coef_[0] which represents the importance of each feature.\n",
    "# What does importance mean in this context?\n",
    "# Some words are more important than others for the model.\n",
    "# It's nothing personal, just that spam emails tend to contain some words more frequently.\n",
    "# This indicates to the model that having that word would make a new email more likely to be spam.\n",
    "# TODO\n",
    "\n",
    "\n",
    "# Iterate over importance and find the top 10 positive features with the largest magnitude.\n",
    "# Similarly, find the top 10 negative features with the largest magnitude.\n",
    "# Positive features correspond to spam. Negative features correspond to ham.\n",
    "# You will see that `http` is the strongest feature that corresponds to spam emails. \n",
    "# It makes sense. Spam emails often want you to click on a link.\n",
    "# TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d267e7ad",
   "metadata": {
    "id": "d267e7ad"
   },
   "source": [
    "Submission\n",
    "1. Upload the jupyter notebook to Forage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LI4u_ZUGToDQ",
   "metadata": {
    "id": "LI4u_ZUGToDQ"
   },
   "source": [
    "All Done!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "task3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
